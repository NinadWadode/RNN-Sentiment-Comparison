RNN Sentiment Comparison

Comparative analysis of RNN, LSTM, and BiLSTM architectures on the IMDb 50k sentiment dataset. The project runs a controlled grid over activation (relu, tanh, sigmoid), optimizer (adam, sgd, rmsprop), sequence length (25, 50, 100), and gradient clipping (off/on). It logs metrics, aggregates, and report-ready plots. Works both as a pure Python CLI and in a Colab/Notebook.

Features

Clean preprocessing: lowercasing, punctuation stripping, NLTK tokenization, top-10k vocab, fixed sequence lengths.

Models: RNN, LSTM, BiLSTM with a shared backbone (100-dim embeddings, 2 layers, hidden size 64, dropout 0.3–0.5).

Full factorial grid: architecture × activation × optimizer × length × clipping.

Metrics: Accuracy, Macro-F1, average epoch time. Optional PR/ROC, calibration, confusion matrices.

Reproducibility: fixed seeds for Python, NumPy, PyTorch.

Outputs: metrics.csv, summary tables, aggregates, and plots saved under results/.

Repository Structure
rnn-sentiment-comparison/
├─ data/
│  ├─ IMDB_Dataset.csv           # add manually (not tracked)
│  ├─ vocab.json                 # generated by preprocess.py
│  └─ dataset_stats.json         # generated by preprocess.py
├─ src/
│  ├─ preprocess.py              # cleaning, tokenization, vocab, arrays
│  ├─ models.py                  # RNN/LSTM/BiLSTM definitions
│  ├─ train.py                   # full grid training
│  ├─ evaluate.py                # metrics tables + core plots
│  └─ visualize.py               # extra plots; save + (if in notebook) display
├─ results/
│  ├─ metrics.csv                # all run rows appended
│  ├─ summary_rounded.csv        # compact table for report
│  ├─ best_worst.json            # best/worst run metadata
│  ├─ aggregates/                # analysis CSVs
│  ├─ plots/                     # PNG figures
│  ├─ preds/                     # per-run predictions (optional)
│  └─ logs/                      # per-run training logs (optional)
├─ requirements.txt
└─ README.md

Setup
Python

Python 3.10 or 3.11 recommended

OS: Linux, macOS, or Windows

Create a virtual environment
python -m venv .venv
# macOS/Linux
source .venv/bin/activate
# Windows
.\.venv\Scripts\activate

Install dependencies
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt', quiet=True)"


If using CUDA, install a matching PyTorch build from https://pytorch.org
.

Data

Download the IMDb 50k CSV (Kaggle) and place it at:

data/IMDB_Dataset.csv


Expected columns: review (text) and sentiment with values positive or negative.

Reproducibility

Scripts set:

torch.manual_seed(42)
numpy.random.seed(42)
random.seed(42)


For exact reproducibility across machines, keep Python and dependency versions aligned.

Usage (CLI)

If running in Colab, prefix commands with ! and adjust paths (e.g., /content/rnn-sentiment-comparison).

1) Preprocess
python src/preprocess.py \
  --data_csv data/IMDB_Dataset.csv \
  --out_dir data \
  --seq_lengths 25 50 100


Outputs:
data/vocab.json, data/dataset_stats.json, data/X_train_len{25,50,100}.npy, data/X_test_len{25,50,100}.npy, data/y_train.npy, data/y_test.npy

2) Train the grid
# GPU
python src/train.py --device cuda --epochs 5
# CPU
python src/train.py --device cpu --epochs 5


Key outputs:

results/metrics.csv (appends rows per config)

results/logs/<run_id>.csv (loss per epoch)

results/preds/<run_id>.csv (y_true, y_prob, y_pred)

3) Evaluate (tables + required plots)
python src/evaluate.py


Key outputs:
results/summary_rounded.csv, results/best_worst.json, results/aggregates/*.csv, results/plots/*.png

4) Extra visuals
python src/visualize.py --base .


Adds more figures to results/plots/. If you run this inside a notebook with %run, it will also display plots inline.

Notebook Workflow (Colab/Jupyter)

Open or clone the project into the environment. In Colab:

%cd /content
# if zipped:
# from google.colab import files; files.upload()
# !unzip -q rnn-sentiment-comparison.zip
%cd /content/rnn-sentiment-comparison


Install requirements in the notebook:

!pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt', quiet=True)"


Put the dataset at:

/content/rnn-sentiment-comparison/data/IMDB_Dataset.csv


Run preprocessing, training, evaluation:

!python src/preprocess.py --data_csv data/IMDB_Dataset.csv --out_dir data --seq_lengths 25 50 100
!python src/train.py --device cuda --epochs 5   # or --device cpu
!python src/evaluate.py


Visualize (display inline):

%run src/visualize.py --base /content/rnn-sentiment-comparison

Expected Runtime

Observed on Colab T4 GPU (approximate, per epoch, batch=32):

LSTM @ L=100: ~2.7–3.0 s/epoch

BiLSTM @ L=100: ~3.4–3.8 s/epoch

The full grid is: 3 (arch) × 3 (act) × 3 (opt) × 3 (len) × 2 (clip) = 162 configs × epochs.
If time is limited, keep --epochs 5 --device cuda or subset the grid in src/train.py.

Outputs

Tables

results/metrics.csv — one row per configuration

results/summary_rounded.csv — compact table for the report

results/aggregates/*.csv — grouped means (e.g., by model, optimizer×length, clipping deltas)

results/best_worst.json — metadata for top/bottom runs

Figures (results/plots/)

f1_vs_len_by_optimizer_<arch>.png

f1_vs_len_by_activation_<arch>.png

pareto_f1_time.png

delta_f1_clipping.png

heatmap_f1_<arch>.png

Best/worst diagnostics: roc_best.png, pr_best.png, cm_best.png, loss_best.png, threshold_sweep_best.png, etc.

Run artifacts (optional)

results/preds/<run_id>.csv — test predictions and probabilities

results/logs/<run_id>.csv — per-epoch loss

Troubleshooting

NLTK resource error:
python -c "import nltk; nltk.download('punkt', quiet=True)"

No CUDA found:
use --device cpu

Large files in Git:
keep raw CSV and .npy out of Git with .gitignore (included). Regenerate locally when needed.

License and Data

Dataset: IMDb Movie Review Dataset (Kaggle).
